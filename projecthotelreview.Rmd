---
title: "Project hotel review"
author: "Dharani, Nalabolu"
date: "11/28/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
Hotel_Reviews <- read.csv("~/Downloads/Hotel_Reviews.csv")
hotel<-Hotel_Reviews
library(NLP)
library(caret)
library(tidyverse)
library(tidytext)
library(plyr)
library(dplyr)
library(sentimentr)
library(SnowballC)
library(tm)
library(RColorBrewer)
library(ROAuth)
library(wordcloud)
library(corpus)
library(ggrepel)
theme_set(theme_classic())

hotel.df <- hotel
str(hotel.df)
hotel.df <- hotel.df[1:1000, ]
hotel.df$reviews = paste (hotel.df$Negative_Review, hotel.df$Positive_Review)
set.seed(1207)
index <- 1:nrow(hotel.df)
```

# partition data into training and validation
```{r}
training.index <- sample(index,trunc(length(index)*0.8))
training.df <- hotel.df$reviews[training.index]
validation.df <- hotel.df$reviews[-training.index]
```

#introduce corpus

```{r}
corpus<-corpus(VectorSource(hotel.df$reviews))
#text cleaning
View(corpus)
#convert the text to lower case
corpus <- tm_map(corpus,content_transformer(tolower))
inspect(corpus[1:20])
#remove nuymbers
corpus<- tm_map(corpus,removeNumbers)
inspect(corpus[1:20])
#remove english common stopwords
corpus<- tm_map(corpus,removeWords,stopwords("english"))
inspect(corpus[1:20])
#remove punctuation
corpus<-tm_map(corpus,removePunctuation)
inspect(corpus[1:20])
#remove extra whitespaces
corpus<-tm_map(corpus,stripWhitespace)
inspect(corpus[1:20])

#load library
library(SnowballC)
#Stem document
corpus <- tm_map(corpus,stemDocument)
writeLines(as.character(corpus[[30]]))
dtm <- DocumentTermMatrix(corpus)
dtm
inspect(dtm[1:2,1000:1005])
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq,decreasing=TRUE)
freq[head(ord)]
#inspect least frequently occurring terms
freq[tail(ord)]
dtmr <-DocumentTermMatrix(corpus, control=list(wordLengths=c(4, 20),
                                               bounds = list(global = c(3,27))))
dtmr
freqr <- colSums(as.matrix(dtmr))
#length should be total number of terms
length(freqr)
#create sort order (asc)
ordr <- order(freqr,decreasing=TRUE)
#inspect most frequently occurring terms
freqr[head(ordr)]
#inspect least frequently occurring terms
freqr[tail(ordr)]
findFreqTerms(dtmr,lowfreq=20)

wf=data.frame(term=names(freqr),occurrences=freqr)
library(ggplot2)
p <- ggplot(subset(wf, freqr>20), aes(term, occurrences))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
review <- iconv (freqr)
```

#sentimental analysis

```{r}
library(syuzhet)
library(lubridate)
s <- get_nrc_sentiment(hotel.df$reviews)
barplot(colSums(s), las=2, col= rainbow(10), ylab= 'Count', main = 'sentiment analysis of Hotel Review')

```

#clustering
```{r}
v <- as.matrix(dtm)
d<- dist(v)
#rhierarchical clustering using Ward’s method
groups <- hclust(d,method="ward.D")
#plot dendogram
plot(groups, hang=1)
rect.hclust(groups,2)
#k means algorithm, 10 clusters
kfit <- kmeans(d, 10, nstart=100)
#plot – need library cluster
library(cluster)
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
#kmeans – determine the optimum number of clusters
wss <- 2:29
for (i in 2:29) wss[i] <- sum(kmeans(d,centers=i,nstart=25)$withinss)
plot(2:29, wss[2:29], type="b", xlab="Number of Clusters",ylab="Within groups sum of squares")
```

